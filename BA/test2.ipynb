{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from featureBA.src.model_philipp import sparse3DBA\n",
    "from featureBA.src.utils import sobel_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"toy_example/data/toyexample_1_data.p\", 'rb'))\n",
    "img = cv2.imread(\"toy_example/data/toyexample_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_perturbed = np.array([[math.cos(10*math.pi/180), -math.sin(10*math.pi/180), 0, 0],\n",
    "             [math.sin(10*math.pi/180), math.cos(10*math.pi/180), 0, 0],\n",
    "             [0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['coords'] = np.around(data['2d_points']).astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_perturbed = np.dot(data['K'], T_perturbed)\n",
    "projected_2d = np.dot(P_perturbed, np.concatenate((data['3d_points'], np.ones(len(data['3d_points']))[:, None]),-1).T)\n",
    "projected_2d = (projected_2d.T/projected_2d.T[:,2,None])[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_2d = np.around(projected_2d)\n",
    "coords_2d = coords_2d.astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [255 265]\n",
      "1 [253 268]\n",
      "2 [236 250]\n",
      "3 [259 268]\n",
      "4 [246 245]\n",
      "5 [258 265]\n",
      "6 [265 244]\n"
     ]
    }
   ],
   "source": [
    "img = img.astype('uint8')\n",
    "for i, p in enumerate(coords_2d):\n",
    "    print(i, p)\n",
    "    cv2.circle(img, tuple(p), 1, (128, 128, 0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('perturbed_points.png', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from featureBA.src.model import sparse3DBA\n",
    "from featureBA.src.utils import sobel_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"toy_example/data/toyexample_1.png\", 0)\n",
    "img = img.astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_torch = torch.from_numpy(img)[None,...]\n",
    "grad_x, grad_y = sobel_filter(img_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [174 149]\n",
      "1 [128 101]\n",
      "2 [ 90 154]\n",
      "3 [ 70 164]\n",
      "4 [155 165]\n",
      "5 [135 112]\n",
      "6 [175 132]\n"
     ]
    }
   ],
   "source": [
    "img = grad_y.numpy().reshape(img.shape).astype('uint8')\n",
    "for i, p in enumerate(coords_2d):\n",
    "    print(i, p)\n",
    "    cv2.circle(img, tuple(p), 1, (128, 128, 0), -1)\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"grad_x.png\", grad_x.numpy().reshape(img.shape).astype('uint8'))\n",
    "cv2.imwrite(\"grad_y.png\", grad_y.numpy().reshape(img.shape).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts3D = torch.from_numpy(data['3d_points'][:,:3])\n",
    "ref2d = torch.from_numpy(np.flip(data['coords'], axis = 1).copy())\n",
    "feature_ref = torch.cat([img_torch[:, i, j].unsqueeze(0) for i, j in zip(ref2d[:,0], ref2d[:,1])]).type(torch.DoubleTensor)\n",
    "feature_map_query = img_torch.type(torch.DoubleTensor)\n",
    "R_init, t_init = torch.from_numpy(T_perturbed[:, :3]), torch.from_numpy(T_perturbed[:, 3])\n",
    "feature_grad_x = grad_x\n",
    "feature_grad_y = grad_y\n",
    "K = torch.from_numpy(data['K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from featureBA.src.utils import (from_homogeneous, to_homogeneous,\n",
    "                                 batched_eye_like, skew_symmetric, so3exp_map)\n",
    "\n",
    "from featureBA.src.utils import squared_loss, scaled_loss\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "def optimizer_step(g, H, lambda_=0):\n",
    "    \"\"\"One optimization step with Gauss-Newton or Levenberg-Marquardt.\n",
    "    Args:\n",
    "        g: batched gradient tensor of size (..., N).\n",
    "        H: batched hessian tensor of size (..., N, N).\n",
    "        lambda_: damping factor for LM (use GN if lambda_=0).\n",
    "    \"\"\"\n",
    "    if lambda_:  # LM instead of GN\n",
    "        D = (H.diagonal(dim1=-2, dim2=-1) + 1e-9).diag_embed()\n",
    "        H = H + D*lambda_\n",
    "    try:\n",
    "        P = torch.inverse(H)\n",
    "    except RuntimeError as e:\n",
    "        logging.warning(f'Determinant: {torch.det(H)}')\n",
    "        raise e\n",
    "    delta = -(P @ g[..., None])[..., 0]\n",
    "    return delta\n",
    "\n",
    "def indexing_(feature_map, points):\n",
    "    '''\n",
    "    Function gives x and y gradients for 3D points in camera frame.\n",
    "\n",
    "    inputs: (All pytorch tensors)\n",
    "    @feature_map : x gradient of the feature map (CxHxW)\n",
    "    @points : pixel coordinates of points (Nx2)\n",
    "\n",
    "    outputs: \n",
    "    features : features for the points (NxC)\n",
    "    '''\n",
    "\n",
    "    features = torch.cat([feature_map[:, i, j].unsqueeze(0) for i, j in zip(points[:,0], points[:,1])])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "loss_fn = squared_loss\n",
    "lambda_ = 0.01\n",
    "\n",
    "R = R_init\n",
    "t = t_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[173, 146],\n",
      "        [126,  96],\n",
      "        [100, 155],\n",
      "        [ 89, 164],\n",
      "        [155, 158],\n",
      "        [135, 111],\n",
      "        [178, 130]], dtype=torch.int32)\n",
      "tensor([[-115.],\n",
      "        [ -88.],\n",
      "        [-132.],\n",
      "        [-136.],\n",
      "        [-113.],\n",
      "        [ -31.],\n",
      "        [-124.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "p_3d_1 = (torch.mm(R, pts3D.T).T + t)\n",
    "p_proj_1 = torch.round(from_homogeneous(torch.mm(K, p_3d_1.T).T)).type(torch.IntTensor)-1\n",
    "print(p_proj_1)\n",
    "error = indexing_(feature_map_query, torch.flip(p_proj_1,(1,))) - feature_ref\n",
    "print((error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([173, 146], dtype=torch.int32)\n",
      "1 tensor([127,  96], dtype=torch.int32)\n",
      "2 tensor([ 99, 154], dtype=torch.int32)\n",
      "3 tensor([ 88, 162], dtype=torch.int32)\n",
      "4 tensor([154, 159], dtype=torch.int32)\n",
      "5 tensor([136, 110], dtype=torch.int32)\n",
      "6 tensor([178, 130], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"toy_example/data/toyexample_1.png\", 0)\n",
    "img = img.astype('uint8')\n",
    "for i, p in enumerate(p_proj_1):\n",
    "    print(i, p)\n",
    "    cv2.circle(img, tuple(p.numpy()), 1, (128, 128, 0), -1)\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# cv2.imwrite(\"iter11.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-115.],\n",
      "        [ -88.],\n",
      "        [-132.],\n",
      "        [-136.],\n",
      "        [-113.],\n",
      "        [ -31.],\n",
      "        [-124.]], dtype=torch.float64)\n",
      "Iter  0 6142.5\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "verbose = 1\n",
    "p_3d_1 = torch.mm(R, pts3D.T).T + t\n",
    "p_proj_1 = torch.round(from_homogeneous(torch.mm(K, p_3d_1.T).T)).type(torch.IntTensor)-1\n",
    "error = indexing_(feature_map_query, torch.flip(p_proj_1,(1,))) - feature_ref\n",
    "# error = torch.flip(p_proj_1, (1,)) - ref2d\n",
    "error = error.type(torch.DoubleTensor)\n",
    "print(error)\n",
    "cost = 0.5*(error**2).sum(-1)\n",
    "\n",
    "if i == 0:\n",
    "    prev_cost = cost.mean(-1)\n",
    "if verbose:\n",
    "    print('Iter ', i, cost.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_p_T = torch.cat([\n",
    "    batched_eye_like(p_3d_1, 3), -skew_symmetric(p_3d_1)], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = p_3d_1.shape[:-1]\n",
    "o, z = p_3d_1.new_ones(shape), p_3d_1.new_zeros(shape)\n",
    "J_e_p = torch.stack([\n",
    "    K[0,0]*o, z, -K[0,0]*p_3d_1[..., 0] / p_3d_1[..., 2],\n",
    "    z, K[1,1]*o, -K[1,1]*p_3d_1[..., 1] / p_3d_1[..., 2],\n",
    "], dim=-1).reshape(shape+(2, 3)) / p_3d_1[..., 2, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x_points = indexing_(feature_grad_x, torch.flip(p_proj_1,(1,)))\n",
    "grad_y_points = indexing_(feature_grad_y, torch.flip(p_proj_1,(1,)))\n",
    "J_p_F = torch.cat((grad_x_points[..., None], grad_y_points[...,None]), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_e_T = J_p_F @ J_e_p @ J_p_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grad = torch.einsum('bij,bi->bj', J_e_T, error)\n",
    "Grad = Grad.sum(-2)  # Grad was ... x N x 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J_e_T\n",
    "Hess = torch.einsum('ijk,ijl->ikl', J, J)\n",
    "# Hess = weights[..., None, None] * Hess\n",
    "Hess = Hess.sum(-3)  # Hess was ... x N x 6 x 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6382],\n",
       "        [-8.9607],\n",
       "        [ 0.2741],\n",
       "        [-0.0935],\n",
       "        [ 0.0430],\n",
       "        [ 0.5951]], dtype=torch.float64)"
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.inverse(Hess), Grad[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = -0.1*torch.matmul(torch.inverse(Hess), Grad[...,None])[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr is :  tensor([ 0.0015, -0.0012, -0.0218], dtype=torch.float64)\n",
      "dt is :  tensor([ 0.0840,  0.2258, -0.0029], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dt, dw = delta[..., :3], delta[..., 3:6]\n",
    "dr = so3exp_map(dw)\n",
    "print(\"dr is : \", dw)\n",
    "print(\"dt is : \", dt)\n",
    "R_new = dr @ R\n",
    "t_new = dr @ t + dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " R_new  tensor([[ 9.9777e-01, -6.6773e-02,  5.2216e-04],\n",
      "        [ 6.6748e-02,  9.9711e-01, -3.6228e-02],\n",
      "        [ 1.8984e-03,  3.6182e-02,  9.9934e-01]], dtype=torch.float64)\n",
      " t_new  tensor([ 0.6018,  1.0098, -0.0339], dtype=torch.float64)\n",
      "R  tensor([[ 9.9608e-01, -8.8406e-02,  2.4494e-03],\n",
      "        [ 8.8438e-02,  9.9548e-01, -3.4660e-02],\n",
      "        [ 6.2580e-04,  3.4740e-02,  9.9940e-01]], dtype=torch.float64)\n",
      "t  tensor([ 0.5006,  0.7950, -0.0328], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\" R_new \", R_new)\n",
    "print(\" t_new \", t_new)\n",
    "print(\"R \", R)\n",
    "print(\"t \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R, t = R_new, t_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(R.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9848, -0.1736,  0.0000],\n",
       "        [ 0.1736,  0.9848,  0.0000],\n",
       "        [ 0.0000,  0.0000,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sparse3DBA(n_iters = 500, lambda_ = 0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  0 11117.142857142857\n",
      "dr is :  tensor([ 0.0344,  0.0200, -0.0111], dtype=torch.float64)\n",
      "dt is :  tensor([-0.1927, -0.1188, -0.0185], dtype=torch.float64)\n",
      "new cost is  17082.214285714286\n",
      "cost increased, continue with next iteration\n",
      "Iter  1 11117.142857142857\n",
      "dr is :  tensor([ 0.0034,  0.0020, -0.0011], dtype=torch.float64)\n",
      "dt is :  tensor([-0.0193, -0.0119, -0.0019], dtype=torch.float64)\n",
      "new cost is  11221.0\n",
      "cost increased, continue with next iteration\n",
      "Iter  2 11117.142857142857\n",
      "dr is :  tensor([ 0.0003,  0.0002, -0.0001], dtype=torch.float64)\n",
      "dt is :  tensor([-0.0019, -0.0012, -0.0002], dtype=torch.float64)\n",
      "new cost is  10953.214285714286\n",
      "Iter  3 10953.214285714286\n",
      "dr is :  tensor([-0.0398, -0.0235, -0.0225], dtype=torch.float64)\n",
      "dt is :  tensor([ 0.2866,  0.0776, -0.1852], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 275 is out of bounds for dimension 1 with size 256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-613-bdf9ba455201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_grad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_grad_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Personal/Study/Sem2/3D Vision/Phorometric-BA/BA/featureBA/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pts3D, feature_ref, feature_map_query, feature_grad_x, feature_grad_y, K, R_init, t_init, confidence, scale)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mnew_3d_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts3D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mnew_proj_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_homogeneous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_3d_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mnew_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexing_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_proj_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfeature_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mnew_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_error\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# new_cost = scaled_loss(new_cost, self.loss_fn, scale[..., None])[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Personal/Study/Sem2/3D Vision/Phorometric-BA/BA/featureBA/src/model.py\u001b[0m in \u001b[0;36mindexing_\u001b[0;34m(feature_map, points)\u001b[0m\n\u001b[1;32m     39\u001b[0m     '''\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Personal/Study/Sem2/3D Vision/Phorometric-BA/BA/featureBA/src/model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m     '''\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 275 is out of bounds for dimension 1 with size 256"
     ]
    }
   ],
   "source": [
    "R, t = model(pts3D, feature_ref, feature_map_query, feature_grad_x, feature_grad_y, K, R_init, t_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.9970e-01, -2.4542e-02,  1.0347e-04],\n",
       "        [ 2.4541e-02,  9.9969e-01,  3.6830e-03],\n",
       "        [-1.9383e-04, -3.6794e-03,  9.9999e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0196, -0.0055,  0.0032], dtype=torch.float64)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.13206942, 1.11894092, 0.57274033, 1.24841041, 0.79335565,\n",
       "         0.46561771],\n",
       "        [1.2855546 , 1.19792658, 1.26153542, 1.5414339 , 1.17330447,\n",
       "         1.04735982]]])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
