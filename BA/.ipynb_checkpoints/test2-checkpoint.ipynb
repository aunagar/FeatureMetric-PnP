{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from featureBA.src.model import sparse3DBA\n",
    "from featureBA.src.utils import sobel_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"toy_example/data/toyexample_1_data.p\", 'rb'))\n",
    "img = cv2.imread(\"toy_example/data/toyexample_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_perturbed = np.array([[math.cos(10*math.pi/180), -math.sin(10*math.pi/180), 0, 0],\n",
    "             [math.sin(10*math.pi/180), math.cos(10*math.pi/180), 0, 0],\n",
    "             [0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['coords'] = np.around(data['2d_points']).astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_perturbed = np.dot(data['K'], T_perturbed)\n",
    "projected_2d = np.dot(P_perturbed, np.concatenate((data['3d_points'], np.ones(len(data['3d_points']))[:, None]),-1).T)\n",
    "projected_2d = (projected_2d.T/projected_2d.T[:,2,None])[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_2d = np.around(projected_2d)\n",
    "coords_2d = coords_2d.astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [136 151]\n"
     ]
    }
   ],
   "source": [
    "img = img.astype('uint8')\n",
    "for i, p in enumerate(coords_2d):\n",
    "    print(i, p)\n",
    "    cv2.circle(img, tuple(p), 1, (128, 128, 0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('perturbed_points.png', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from featureBA.src.model import sparse3DBA\n",
    "from featureBA.src.utils import sobel_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"toy_example/data/toyexample_1.png\", 0)\n",
    "img = img.astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_torch = torch.from_numpy(img)[None,...]\n",
    "grad_x, grad_y = sobel_filter(img_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [136 151]\n"
     ]
    }
   ],
   "source": [
    "img = grad_y.numpy().reshape(img.shape).astype('uint8')\n",
    "for i, p in enumerate(coords_2d):\n",
    "    print(i, p)\n",
    "    cv2.circle(img, tuple(p), 1, (128, 128, 0), -1)\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"grad_x.png\", grad_x.numpy().reshape(img.shape).astype('uint8'))\n",
    "cv2.imwrite(\"grad_y.png\", grad_y.numpy().reshape(img.shape).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts3D = torch.from_numpy(data['3d_points'][:,:3])\n",
    "ref2d = torch.from_numpy(np.flip(data['coords'], axis = 1).copy())\n",
    "feature_ref = torch.cat([img_torch[:, i, j].unsqueeze(0) for i, j in zip(ref2d[:,0], ref2d[:,1])]).type(torch.DoubleTensor)\n",
    "feature_map_query = img_torch.type(torch.DoubleTensor)\n",
    "R_init, t_init = torch.from_numpy(T_perturbed[:, :3]), torch.from_numpy(T_perturbed[:, 3])\n",
    "feature_grad_x = grad_x\n",
    "feature_grad_y = grad_y\n",
    "K = torch.from_numpy(data['K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from featureBA.src.utils import (from_homogeneous, to_homogeneous,\n",
    "                                 batched_eye_like, skew_symmetric, so3exp_map)\n",
    "\n",
    "from featureBA.src.utils import squared_loss, scaled_loss\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "def optimizer_step(g, H, lambda_=0):\n",
    "    \"\"\"One optimization step with Gauss-Newton or Levenberg-Marquardt.\n",
    "    Args:\n",
    "        g: batched gradient tensor of size (..., N).\n",
    "        H: batched hessian tensor of size (..., N, N).\n",
    "        lambda_: damping factor for LM (use GN if lambda_=0).\n",
    "    \"\"\"\n",
    "    if lambda_:  # LM instead of GN\n",
    "        D = (H.diagonal(dim1=-2, dim2=-1) + 1e-9).diag_embed()\n",
    "        H = H + D*lambda_\n",
    "    try:\n",
    "        P = torch.inverse(H)\n",
    "    except RuntimeError as e:\n",
    "        logging.warning(f'Determinant: {torch.det(H)}')\n",
    "        raise e\n",
    "    delta = -(P @ g[..., None])[..., 0]\n",
    "    return delta\n",
    "\n",
    "def indexing_(feature_map, points):\n",
    "    '''\n",
    "    Function gives x and y gradients for 3D points in camera frame.\n",
    "\n",
    "    inputs: (All pytorch tensors)\n",
    "    @feature_map : x gradient of the feature map (CxHxW)\n",
    "    @points : pixel coordinates of points (Nx2)\n",
    "\n",
    "    outputs: \n",
    "    features : features for the points (NxC)\n",
    "    '''\n",
    "\n",
    "    features = torch.cat([feature_map[:, i, j].unsqueeze(0) for i, j in zip(points[:,0], points[:,1])])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "loss_fn = squared_loss\n",
    "lambda_ = 0.01\n",
    "\n",
    "R = R_init\n",
    "t = t_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-250.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "p_3d_1 = (torch.mm(R, pts3D.T).T + t)\n",
    "p_proj_1 = torch.round(from_homogeneous(torch.mm(K, p_3d_1.T).T)).type(torch.IntTensor)-1\n",
    "error = indexing_(feature_map_query, torch.flip(p_proj_1,(1,))) - feature_ref\n",
    "print((error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = torch.flip(p_proj_1,(1,)) - ref2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([146, 134], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"toy_example/data/toyexample_1.png\", 0)\n",
    "img = img.astype('uint8')\n",
    "for i, p in enumerate(p_proj_1):\n",
    "    print(i, p)\n",
    "    cv2.circle(img, tuple(p.numpy()), 1, (128, 128, 0), -1)\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-15.,   6.]], dtype=torch.float64)\n",
      "Iter  0 130.5\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "verbose = 1\n",
    "p_3d_1 = torch.mm(R, pts3D.T).T + t\n",
    "p_proj_1 = torch.round(from_homogeneous(torch.mm(K, p_3d_1.T).T)).type(torch.IntTensor)-1\n",
    "# error = indexing_(feature_map_query, torch.flip(p_proj_1,(1,))) - feature_ref\n",
    "error = torch.flip(p_proj_1, (1,)) - ref2d\n",
    "error = error.type(torch.DoubleTensor)\n",
    "print(error)\n",
    "cost = 0.5*(error**2).sum(-1)\n",
    "\n",
    "if i == 0:\n",
    "    prev_cost = cost.mean(-1)\n",
    "if verbose:\n",
    "    print('Iter ', i, cost.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_p_T = torch.cat([\n",
    "    batched_eye_like(p_3d_1, 3), -skew_symmetric(p_3d_1)], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = p_3d_1.shape[:-1]\n",
    "o, z = p_3d_1.new_ones(shape), p_3d_1.new_zeros(shape)\n",
    "J_e_p = torch.stack([\n",
    "    K[0,0]*o, z, -K[0,0]*p_3d_1[..., 0] / p_3d_1[..., 2],\n",
    "    z, K[1,1]*o, -K[1,1]*p_3d_1[..., 1] / p_3d_1[..., 2],\n",
    "], dim=-1).reshape(shape+(2, 3)) / p_3d_1[..., 2, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x_points = indexing_(feature_grad_x, torch.flip(p_proj_1,(1,)))\n",
    "grad_y_points = indexing_(feature_grad_y, torch.flip(p_proj_1,(1,)))\n",
    "J_p_F = torch.cat((grad_x_points[..., None], grad_y_points[...,None]), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_e_T = J_p_F @ J_e_p @ J_p_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  1.3709,   0.0000,  -5.0881, -26.1725,  73.8806,  -7.0515],\n",
      "         [  0.0000,   1.3709,  -1.9333, -14.9447,  26.1725,  18.5581]]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "J_e_T = J_e_p @ J_p_T\n",
    "print(J_e_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grad = torch.einsum('...ijk,...ij->...ik', J_e_T, error)\n",
    "Grad = Grad.sum(-2)  # Grad was ... x N x 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J_e_T\n",
    "Hess = torch.einsum('...ijk,...ijl->...ikl', J, J)\n",
    "# Hess = weights[..., None, None] * Hess\n",
    "Hess = Hess.sum(-3)  # Hess was ... x N x 6 x 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[79.8540],\n",
       "        [19.9733],\n",
       "        [-2.2835],\n",
       "        [ 3.3155],\n",
       "        [-0.7431],\n",
       "        [-0.4048]], dtype=torch.float64)"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.inverse(Hess), Grad[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = -0.1*torch.matmul(torch.inverse(Hess), Grad[...,None])[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr is :  tensor([-0.3316,  0.0743,  0.0405], dtype=torch.float64)\n",
      "dt is :  tensor([-7.9854, -1.9973,  0.2284], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dt, dw = delta[..., :3], delta[..., 3:6]\n",
    "dr = so3exp_map(dw)\n",
    "print(\"dr is : \", dw)\n",
    "print(\"dt is : \", dt)\n",
    "R_new = dr @ R\n",
    "t_new = dr @ t + dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " R_new  tensor([[ 0.8589,  0.5121, -0.0086],\n",
      "        [-0.4722,  0.7984,  0.3737],\n",
      "        [ 0.1982, -0.3169,  0.9275]], dtype=torch.float64)\n",
      " t_new  tensor([-6.3682, -2.3256,  0.6918], dtype=torch.float64)\n",
      "R  tensor([[ 0.8271,  0.5574, -0.0721],\n",
      "        [-0.5549,  0.8302,  0.0533],\n",
      "        [ 0.0895, -0.0041,  0.9960]], dtype=torch.float64)\n",
      "t  tensor([ 1.5656, -0.5440,  0.4368], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\" R_new \", R_new)\n",
    "print(\" t_new \", t_new)\n",
    "print(\"R \", R)\n",
    "print(\"t \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R, t = R_new, t_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sparse3DBA(n_iters = 100, lambda_ = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev cost is  tensor(3042., dtype=torch.float64)\n",
      "J_p_T is  torch.Size([1, 3, 6])\n",
      "J_e_p is  torch.Size([1, 2, 3])\n",
      "J_p_F is  torch.Size([1, 1, 2])\n",
      "dr is :  tensor([ 12.9108, -11.0312,   0.1069], dtype=torch.float64)\n",
      "dt is :  tensor([-128.4801,  -27.8761,  -14.5605], dtype=torch.float64)\n",
      "new cost is  tensor(32512.5000, dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 376 is out of bounds for dimension 2 with size 256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-bdf9ba455201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_grad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_grad_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Personal/Study/Sem2/3D Vision/Phorometric-BA/BA/featureBA/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pts3D, feature_ref, feature_map_query, feature_grad_x, feature_grad_y, K, R_init, t_init, confidence, scale)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mp_proj_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_homogeneous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_3d_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# print(p_proj_1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexing_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_proj_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfeature_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Personal/Study/Sem2/3D Vision/Phorometric-BA/BA/featureBA/src/model.py\u001b[0m in \u001b[0;36mindexing_\u001b[0;34m(feature_map, points)\u001b[0m\n\u001b[1;32m     42\u001b[0m     '''\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Personal/Study/Sem2/3D Vision/Phorometric-BA/BA/featureBA/src/model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     '''\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 376 is out of bounds for dimension 2 with size 256"
     ]
    }
   ],
   "source": [
    "R, t = model(pts3D, feature_ref, feature_map_query, feature_grad_x, feature_grad_y, K, R_init, t_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1564,  0.0554, -0.0017], dtype=torch.float64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.], dtype=torch.float64),\n",
       " tensor([ 0.0588, -0.0126,  0.0119], dtype=torch.float64))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_init, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.13206942, 1.11894092, 0.57274033, 1.24841041, 0.79335565,\n",
       "        0.46561771],\n",
       "       [1.2855546 , 1.19792658, 1.26153542, 1.5414339 , 1.17330447,\n",
       "        1.04735982]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] @ b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.13206942, 1.11894092, 0.57274033, 1.24841041, 0.79335565,\n",
       "         0.46561771],\n",
       "        [1.2855546 , 1.19792658, 1.26153542, 1.5414339 , 1.17330447,\n",
       "         1.04735982]]])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
